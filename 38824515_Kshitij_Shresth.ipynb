{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRHAr4-LBU76",
        "outputId": "5a95fa09-acc7-4256-cd30-4ca02bc96651"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Original Class Distribution ===\n",
            "Sentiment\n",
            "1    29541\n",
            "2     3107\n",
            "3     4278\n",
            "4     5093\n",
            "5    11351\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Balanced Class Distribution ===\n",
            "Sentiment\n",
            "1    29541\n",
            "2    29541\n",
            "3    29541\n",
            "4    29541\n",
            "5    11351\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Dataset Size ===\n",
            "Original rows: 53370, Balanced rows: 129515\n"
          ]
        }
      ],
      "source": [
        "# === Data Loading and Preparation ===\n",
        "!pip install praw\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AdamW\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "import praw\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Loading training datasets\n",
        "cnbc_df = pd.read_csv(\"cnbc_sentiment.csv\")\n",
        "guardian_df = pd.read_csv(\"guardian_sentiment.csv\")\n",
        "reuters_df = pd.read_csv(\"reuters_sentiment.csv\")\n",
        "df = pd.concat([\n",
        "    cnbc_df[['Headlines', 'Sentiment']],\n",
        "    guardian_df[['Headlines', 'Sentiment']],\n",
        "    reuters_df[['Headlines', 'Sentiment']]\n",
        "])\n",
        "df['Sentiment'] = df['Sentiment'].astype(int)\n",
        "\n",
        "# Oversampling minority classes for balanced dataset\n",
        "minority_classes = [2, 3, 4]\n",
        "max_count = df['Sentiment'].value_counts().max()\n",
        "oversampled_dfs = [df[df['Sentiment'] == 1], df[df['Sentiment'] == 5]]\n",
        "for sentiment in minority_classes:\n",
        "    class_df = df[df['Sentiment'] == sentiment]\n",
        "    oversampled_df = class_df.sample(max_count, replace=True, random_state=42)\n",
        "    oversampled_dfs.append(oversampled_df)\n",
        "df_balanced = pd.concat(oversampled_dfs)\n",
        "\n",
        "# Splitting datasets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df_balanced['Headlines'].tolist(),\n",
        "    df_balanced['Sentiment'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df_balanced['Sentiment']\n",
        ")\n",
        "train_texts_full, val_texts_full, train_labels_full, val_labels_full = train_test_split(\n",
        "    df['Headlines'].tolist(),\n",
        "    df['Sentiment'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Defining SentimentDataset (its shared across models)\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer=None, word_to_idx=None, max_length=64):\n",
        "        self.texts = texts\n",
        "        self.labels = [label - 1 for label in labels]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.tokenizer:\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(0),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "                'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "        elif self.word_to_idx:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            indices = [self.word_to_idx.get(token, 0) for token in tokens[:self.max_length]]\n",
        "            indices += [0] * (self.max_length - len(indices)) if len(indices) < self.max_length else indices\n",
        "            return {\n",
        "                'input_ids': torch.tensor(indices, dtype=torch.long),\n",
        "                'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            raise ValueError(\"Must provide tokenizer or word_to_idx\")\n",
        "\n",
        "# Vocabulary for LSTM\n",
        "all_words = [word for text in df_balanced['Headlines'] for word in word_tokenize(text.lower())]\n",
        "vocab = {word: idx + 1 for idx, (word, _) in enumerate(Counter(all_words).most_common(10000))}\n",
        "vocab['<unk>'] = 0\n",
        "all_words_full = [word for text in df['Headlines'] for word in word_tokenize(text.lower())]\n",
        "vocab_full = {word: idx + 1 for idx, (word, _) in enumerate(Counter(all_words_full).most_common(10000))}\n",
        "vocab_full['<unk>'] = 0\n",
        "\n",
        "# Dataset info\n",
        "print(\"=== Original Class Distribution ===\")\n",
        "print(df['Sentiment'].value_counts().sort_index())\n",
        "print(\"\\n=== Balanced Class Distribution ===\")\n",
        "print(df_balanced['Sentiment'].value_counts().sort_index())\n",
        "print(\"\\n=== Dataset Size ===\")\n",
        "print(f\"Original rows: {df.shape[0]}, Balanced rows: {df_balanced.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAxoywASNzIO"
      },
      "outputs": [],
      "source": [
        "# === DistilBERT Model and Combined Metrics ===\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5).to(device)\n",
        "model_weighted = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5).to(device)\n",
        "\n",
        "# Training setup (Undersampled)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "class_counts = df_balanced['Sentiment'].value_counts().sort_index()\n",
        "class_weights = torch.tensor([1.0 / count for count in class_counts], dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer=tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "# Training loop at 5 epochs\n",
        "model.train()\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"DistilBERT Undersampled Epoch {epoch+1}/5, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Training setup (Class-Weighted)\n",
        "optimizer_weighted = AdamW(model_weighted.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "scaler_weighted = torch.amp.GradScaler('cuda')\n",
        "class_counts_full = df['Sentiment'].value_counts().sort_index()\n",
        "class_weights_full = torch.tensor([1.0 / count for count in class_counts_full], dtype=torch.float).to(device)\n",
        "criterion_weighted = nn.CrossEntropyLoss(weight=class_weights_full)\n",
        "\n",
        "train_dataset_full = SentimentDataset(train_texts_full, train_labels_full, tokenizer=tokenizer)\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "# Training loop\n",
        "model_weighted.train()\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader_full:\n",
        "        optimizer_weighted.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model_weighted(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion_weighted(outputs.logits, labels)\n",
        "        scaler_weighted.scale(loss).backward()\n",
        "        scaler_weighted.unscale_(optimizer_weighted)\n",
        "        torch.nn.utils.clip_grad_norm_(model_weighted.parameters(), max_norm=1.0)\n",
        "        scaler_weighted.step(optimizer_weighted)\n",
        "        scaler_weighted.update()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader_full)\n",
        "    print(f\"DistilBERT Class-Weighted Epoch {epoch+1}/5, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Combined evaluation function\n",
        "def evaluate_model(model, tokenizer, texts, labels, model_name):\n",
        "    eval_dataset = SentimentDataset(texts, labels, tokenizer=tokenizer)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "    model.eval()\n",
        "    all_preds_regression, all_preds_classification, all_labels = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            probabilities = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            preds_regression = [sum(i * prob for i, prob in enumerate(prob)) + 1 for prob in probabilities]\n",
        "            preds_classification = np.argmax(probabilities, axis=1)\n",
        "            all_preds_regression.extend(preds_regression)\n",
        "            all_preds_classification.extend(preds_classification)\n",
        "            all_labels.extend([label + 1 for label in labels.cpu().numpy()])\n",
        "    # Regression metrics\n",
        "    mse = mean_squared_error(all_labels, all_preds_regression)\n",
        "    mae = mean_absolute_error(all_labels, all_preds_regression)\n",
        "    r2 = r2_score(all_labels, all_preds_regression)\n",
        "    # Classification metrics\n",
        "    accuracy = accuracy_score([label - 1 for label in all_labels], all_preds_classification)\n",
        "    precision = precision_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    recall = recall_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    f1 = f1_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    print(f\"\\n=== {model_name} Combined Metrics ===\")\n",
        "    print(\"Regression Metrics:\")\n",
        "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "    print(\"Classification Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "    print(\"Sample Predictions vs True Labels:\")\n",
        "    for i in range(min(5, len(all_preds_regression))):\n",
        "        print(f\"Pred: {all_preds_regression[i]:.2f}, True: {all_labels[i]:.2f}\")\n",
        "\n",
        "# Evaluate both models\n",
        "evaluate_model(model, tokenizer, val_texts, val_labels, \"DistilBERT Undersampled\")\n",
        "evaluate_model(model_weighted, tokenizer, val_texts_full, val_labels_full, \"DistilBERT Class-Weighted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRBeZfgNN2W8"
      },
      "outputs": [],
      "source": [
        "# === MiniLM Model and Combined Metrics ===\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loading tokenizer and the model\n",
        "tokenizer_mini = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
        "model_mini = AutoModelForSequenceClassification.from_pretrained('microsoft/MiniLM-L12-H384-uncased', num_labels=5).to(device)\n",
        "\n",
        "# Training setup\n",
        "optimizer_mini = AdamW(model_mini.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "scaler_mini = torch.amp.GradScaler('cuda')\n",
        "class_counts = df_balanced['Sentiment'].value_counts().sort_index()\n",
        "class_weights = torch.tensor([1.0 / count for count in class_counts], dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer=tokenizer_mini)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "# Training loop fixed at 5 epochs\n",
        "model_mini.train()\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer_mini.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model_mini(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "        scaler_mini.scale(loss).backward()\n",
        "        scaler_mini.unscale_(optimizer_mini)\n",
        "        torch.nn.utils.clip_grad_norm_(model_mini.parameters(), max_norm=1.0)\n",
        "        scaler_mini.step(optimizer_mini)\n",
        "        scaler_mini.update()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"MiniLM Undersampled Epoch {epoch+1}/5, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Combined evaluation function\n",
        "def evaluate_model(model, tokenizer, texts, labels, model_name):\n",
        "    eval_dataset = SentimentDataset(texts, labels, tokenizer=tokenizer)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "    model.eval()\n",
        "    all_preds_regression, all_preds_classification, all_labels = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            probabilities = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            preds_regression = [sum(i * prob for i, prob in enumerate(prob)) + 1 for prob in probabilities]\n",
        "            preds_classification = np.argmax(probabilities, axis=1)\n",
        "            all_preds_regression.extend(preds_regression)\n",
        "            all_preds_classification.extend(preds_classification)\n",
        "            all_labels.extend([label + 1 for label in labels.cpu().numpy()])\n",
        "\n",
        "# Regression metrics\n",
        "    mse = mean_squared_error(all_labels, all_preds_regression)\n",
        "    mae = mean_absolute_error(all_labels, all_preds_regression)\n",
        "    r2 = r2_score(all_labels, all_preds_regression)\n",
        "\n",
        "# Classification metrics\n",
        "    accuracy = accuracy_score([label - 1 for label in all_labels], all_preds_classification)\n",
        "    precision = precision_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    recall = recall_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    f1 = f1_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    print(f\"\\n=== {model_name} Combined Metrics ===\")\n",
        "    print(\"Regression Metrics:\")\n",
        "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "    print(\"Classification Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "    print(\"Sample Predictions vs True Labels:\")\n",
        "    for i in range(min(5, len(all_preds_regression))):\n",
        "        print(f\"Pred: {all_preds_regression[i]:.2f}, True: {all_labels[i]:.2f}\")\n",
        "\n",
        "# Model evaluation\n",
        "evaluate_model(model_mini, tokenizer_mini, val_texts, val_labels, \"MiniLM Undersampled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqHu6X2KOSqO"
      },
      "outputs": [],
      "source": [
        "# === LSTM Model and Combined Metrics ===\n",
        "\n",
        "# Downloading GloVe embeddings which give better word representations than random initialization\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d glove\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\"\"\"\n",
        "Loading GloVe embeddings from the downloaded file\n",
        "- I’m using 100-dimensional vectors because they’re a good balance of size and quality\n",
        "- Storing them in a dictionary for quick lookup\n",
        "\"\"\"\n",
        "glove_path = 'glove/glove.6B.100d.txt'\n",
        "embeddings_index = {}\n",
        "with open(glove_path, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "embedding_dim = 100\n",
        "\n",
        "# These matrices will map my vocab words to GloVe vectors\n",
        "embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))\n",
        "for word, idx in vocab.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "embedding_matrix_full = np.zeros((len(vocab_full) + 1, embedding_dim))\n",
        "for word, idx in vocab_full.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_full[idx] = embedding_vector\n",
        "\n",
        "# Defining my LSTM model class\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size, embedding_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "# Embedding layer with pre-trained GloVe vectors, not frozen so it can fine-tune\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=False)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "# Forward pass for predictions\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        output, (hidden, _) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        return self.fc(self.dropout(hidden))\n",
        "\n",
        "# Setting up the undersampled LSTM model\n",
        "hidden_dim = 256\n",
        "output_dim = 5\n",
        "model_lstm_under = LSTMClassifier(embedding_dim, hidden_dim, output_dim, len(vocab) + 1, embedding_matrix).to(device)\n",
        "optimizer = torch.optim.Adam(model_lstm_under.parameters(), lr=0.001)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "class_counts = df_balanced['Sentiment'].value_counts().sort_index()\n",
        "class_weights = torch.tensor([1.0 / count for count in class_counts], dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Preparing training data for undersampled model\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, word_to_idx=vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Training loop for undersampled LSTM with 5 epochs\n",
        "model_lstm_under.train()\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model_lstm_under(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Undersampling LSTM Epoch {epoch+1}/5, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Setting up the class-weighted LSTM model\n",
        "model_lstm_weighted = LSTMClassifier(embedding_dim, hidden_dim, output_dim, len(vocab_full) + 1, embedding_matrix_full).to(device)\n",
        "optimizer_weighted = torch.optim.Adam(model_lstm_weighted.parameters(), lr=0.001)\n",
        "scaler_weighted = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# Class weights for the full dataset\n",
        "class_counts_full = df['Sentiment'].value_counts().sort_index()\n",
        "class_weights_full = torch.tensor([1.0 / count for count in class_counts_full], dtype=torch.float).to(device)\n",
        "criterion_weighted = nn.CrossEntropyLoss(weight=class_weights_full)\n",
        "\n",
        "# Preparing training data for class-weighted model\n",
        "train_dataset_full = SentimentDataset(train_texts_full, train_labels_full, word_to_idx=vocab_full)\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Training loop for class-weighted LSTM\n",
        "model_lstm_weighted.train()\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader_full:\n",
        "        optimizer_weighted.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model_lstm_weighted(input_ids)\n",
        "            loss = criterion_weighted(outputs, labels)\n",
        "        scaler_weighted.scale(loss).backward()\n",
        "        scaler_weighted.step(optimizer_weighted)\n",
        "        scaler_weighted.update()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Class-Weighted LSTM Epoch {epoch+1}/5, Loss: {total_loss/len(train_loader_full):.4f}\")\n",
        "\n",
        "# Function to evaluate LSTM models with combined metrics\n",
        "def evaluate_lstm(model, texts, labels, word_to_idx, model_name):\n",
        "    eval_dataset = SentimentDataset(texts, labels, word_to_idx=word_to_idx)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "    model.eval()\n",
        "    all_preds_regression, all_preds_classification, all_labels = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = model(input_ids)\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "            preds_regression = [sum(i * prob for i, prob in enumerate(prob)) + 1 for prob in probabilities]\n",
        "            preds_classification = np.argmax(probabilities, axis=1)\n",
        "            all_preds_regression.extend(preds_regression)\n",
        "            all_preds_classification.extend(preds_classification)\n",
        "            all_labels.extend([label + 1 for label in labels.cpu().numpy()])\n",
        "\n",
        "# Regression metrics to see how close predictions are to true values\n",
        "    mse = mean_squared_error(all_labels, all_preds_regression)\n",
        "    mae = mean_absolute_error(all_labels, all_preds_regression)\n",
        "    r2 = r2_score(all_labels, all_preds_regression)\n",
        "\n",
        "# Classification metrics for exact class matches\n",
        "    accuracy = accuracy_score([label - 1 for label in all_labels], all_preds_classification)\n",
        "    precision = precision_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    recall = recall_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    f1 = f1_score([label - 1 for label in all_labels], all_preds_classification, average='weighted')\n",
        "    print(f\"\\n=== {model_name} Combined Metrics ===\")\n",
        "    print(\"Regression Metrics:\")\n",
        "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "    print(\"Classification Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "    print(\"Sample Predictions vs True Labels:\")\n",
        "    for i in range(min(5, len(all_preds_regression))):\n",
        "        print(f\"Pred: {all_preds_regression[i]:.2f}, True: {all_labels[i]:.2f}\")\n",
        "\n",
        "# Evaluating both LSTM models\n",
        "evaluate_lstm(model_lstm_under, val_texts, val_labels, vocab, \"LSTM Undersampled\")\n",
        "evaluate_lstm(model_lstm_weighted, val_texts_full, val_labels_full, vocab_full, \"LSTM Class-Weighted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxD7RQbLOZNA"
      },
      "outputs": [],
      "source": [
        "# === Real-Time Gold Sentiment and Trading Signals (DistilBERT, MiniLM, and LSTM) ===\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loading tokenizers for my transformer models\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer_mini = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
        "\n",
        "\"\"\"\n",
        "Loading DistilBERT models\n",
        "- I trained two versions: undersampled and class-weighted\n",
        "- They predict sentiment on a 1-5 scale, so num_labels=5\n",
        "- Moving them to the device for computation\n",
        "\"\"\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5).to(device)\n",
        "model_weighted = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5).to(device)\n",
        "try:\n",
        "    # Loading weights is important\n",
        "    model.load_state_dict(torch.load('distilbert_under_model.pth', map_location=device))\n",
        "    model_weighted.load_state_dict(torch.load('distilbert_weighted_model.pth', map_location=device))\n",
        "    print(\"DistilBERT models loaded with trained weights.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: DistilBERT trained weights not found. Predictions will be random unless trained.\")\n",
        "except NameError:\n",
        "    print(\"DistilBERT models not found in memory.\")\n",
        "\n",
        "# Loading MiniLM model and weights\n",
        "model_mini = AutoModelForSequenceClassification.from_pretrained('microsoft/MiniLM-L12-H384-uncased', num_labels=5).to(device)\n",
        "try:\n",
        "    # Adjust path if saved differently in Cell 3\n",
        "    model_mini.load_state_dict(torch.load('minilm_model.pth', map_location=device))\n",
        "    print(\"MiniLM model loaded with trained weights.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: MiniLM trained weights not found. Predictions will be random unless trained.\")\n",
        "except NameError:\n",
        "    print(\"MiniLM model not found in memory.\")\n",
        "\n",
        "# Setting up LSTM parameters\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 5\n",
        "\n",
        "# Preparing embedding matrices for LSTM for undersampled and full vocabulary\n",
        "try:\n",
        "    embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))\n",
        "    embedding_matrix_full = np.zeros((len(vocab_full) + 1, embedding_dim))\n",
        "except NameError:\n",
        "    print(\"Vocab not defined.\")\n",
        "    vocab = {}\n",
        "    vocab_full = {}\n",
        "\n",
        "# Reusing the LSTM class\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size, embedding_matrix):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=False)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        output, (hidden, _) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        return self.fc(self.dropout(hidden))\n",
        "\n",
        "# Initializing LSTM models and loading weights\n",
        "try:\n",
        "    model_lstm_under = LSTMClassifier(embedding_dim, hidden_dim, output_dim, len(vocab) + 1, embedding_matrix).to(device)\n",
        "    model_lstm_weighted = LSTMClassifier(embedding_dim, hidden_dim, output_dim, len(vocab_full) + 1, embedding_matrix_full).to(device)\n",
        "    try:\n",
        "        model_lstm_under.load_state_dict(torch.load('lstm_under_model.pth', map_location=device))\n",
        "        model_lstm_weighted.load_state_dict(torch.load('lstm_weighted_model.pth', map_location=device))\n",
        "        print(\"LSTM models loaded with trained weights.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: LSTM trained weights not found. Predictions will be random unless trained.\")\n",
        "except NameError:\n",
        "    print(\"LSTM models not initialized due to missing vocab or embeddings.\")\n",
        "\n",
        "# Function to fetch news from NewsData.io\n",
        "def get_newsdata_articles(query):\n",
        "    api_key = \"\"\n",
        "    url = f\"https://newsdata.io/api/1/news?apikey={api_key}&q={query}&language=en\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get('results', [])\n",
        "        return [article['title'] for article in articles[:5]]\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"NewsData.io Error: {e}. Check API key validity or quota.\")\n",
        "        return []\n",
        "\n",
        "# Function to fetch news from MarketAux\n",
        "def get_marketaux_articles(query):\n",
        "    api_key = \"\"\n",
        "    url = f\"https://api.marketaux.com/v1/news/all?symbols={query}&api_token={api_key}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get('data', [])\n",
        "        return [article['title'] for article in articles[:5]]\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"MarketAux Error: {e}. Check API key or quota.\")\n",
        "        return []\n",
        "\n",
        "# Function to fetch Reddit posts\n",
        "def get_reddit_posts(query):\n",
        "    reddit = praw.Reddit(client_id=\"\",\n",
        "                         client_secret=\"\",\n",
        "                         user_agent=\"\")\n",
        "    try:\n",
        "        subreddit = reddit.subreddit(\"all\")\n",
        "        posts = subreddit.search(query, limit=5)\n",
        "        return [post.title for post in posts]\n",
        "    except Exception as e:\n",
        "        print(f\"Reddit Error: {e}. Check credentials or connectivity.\")\n",
        "        return []\n",
        "\n",
        "# Collecting latest gold-related texts from all sources (gold as a placeholder, works on commodities in general)\n",
        "gold_texts = []\n",
        "gold_texts.extend(get_newsdata_articles(\"gold commodity\"))\n",
        "gold_texts.extend(get_marketaux_articles(\"GC=F\"))\n",
        "gold_texts.extend(get_reddit_posts(\"gold commodity\"))\n",
        "\n",
        "if not gold_texts:\n",
        "    print(\"No gold-related texts fetched. Verify API keys and network connection.\")\n",
        "else:\n",
        "    print(\"\\n=== Latest Gold Commodity Texts ===\")\n",
        "    for i, text in enumerate(gold_texts, 1):\n",
        "        print(f\"{i}. {text}\")\n",
        "\n",
        "# Prediction function for transformer models\n",
        "    def predict_transformer(model, tokenizer, texts):\n",
        "        model.eval()\n",
        "        encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "        input_ids = encodings['input_ids'].to(device)\n",
        "        attention_mask = encodings['attention_mask'].to(device)\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            probabilities = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            preds = [sum(i * prob for i, prob in enumerate(prob)) + 1 for prob in probabilities]\n",
        "        return preds\n",
        "\n",
        "# Prediction function for LSTM\n",
        "    def predict_lstm(model, texts, word_to_idx, max_len=128):\n",
        "        model.eval()\n",
        "        tokenized = [[word_to_idx.get(word, 0) for word in text.lower().split()] for text in texts]\n",
        "        padded = [seq[:max_len] + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in tokenized]\n",
        "        input_ids = torch.tensor(padded, dtype=torch.long).to(device)\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = model(input_ids)\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "            preds = [sum(i * prob for i, prob in enumerate(prob)) + 1 for prob in probabilities]\n",
        "        return preds\n",
        "\n",
        "# Signal mappings\n",
        "    def get_signal(pred):\n",
        "        if pred > 3.5:\n",
        "            return \"Buy\"\n",
        "        elif pred < 2.5:\n",
        "            return \"Sell\"\n",
        "        else:\n",
        "            return \"Hold\"\n",
        "\n",
        "# Prediction and signal generation\n",
        "    models = {\n",
        "        \"DistilBERT Undersampled\": (model, tokenizer),\n",
        "        \"DistilBERT Class-Weighted\": (model_weighted, tokenizer),\n",
        "        \"MiniLM Undersampled\": (model_mini, tokenizer_mini),\n",
        "        \"LSTM Undersampled\": (model_lstm_under, vocab),\n",
        "        \"LSTM Class-Weighted\": (model_lstm_weighted, vocab_full)\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== Trading Signals for Gold Commodity ===\")\n",
        "    for model_name, (model_obj, tok_or_vocab) in models.items():\n",
        "        if \"LSTM\" in model_name:\n",
        "            preds = predict_lstm(model_obj, gold_texts, tok_or_vocab)\n",
        "        else:\n",
        "            preds = predict_transformer(model_obj, tok_or_vocab, gold_texts)\n",
        "        signals = [get_signal(pred) for pred in preds]\n",
        "        avg_pred = np.mean(preds)\n",
        "        avg_signal = get_signal(avg_pred)\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        for i, (text, pred, signal) in enumerate(zip(gold_texts, preds, signals), 1):\n",
        "            print(f\"{i}. {text}\")\n",
        "            print(f\"   Predicted Sentiment: {pred:.2f}, Signal: {signal}\")\n",
        "        print(f\"Average Sentiment: {avg_pred:.2f}, Average Signal: {avg_signal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdqXNKVQnXCg"
      },
      "outputs": [],
      "source": [
        "# === Backtesting and Returns Calculation ===\n",
        "\n",
        "print(\"\\n=== Backtesting and Returns Calculation ===\\n\")\n",
        "\n",
        "# Fetching current price and monthly change from Trading Economics\n",
        "def get_price_data(ticker):\n",
        "    url = \"https://tradingeconomics.com/commodity/gold\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        tables = soup.find_all('table')\n",
        "        target_table = None\n",
        "\n",
        "# Looking for the table with gold data from the website\n",
        "        for table in tables:\n",
        "            if 'Gold' in table.get_text():\n",
        "                target_table = table\n",
        "                break\n",
        "        if not target_table:\n",
        "            print(\"No table containing 'Gold' found.\")\n",
        "            return None, None\n",
        "        headers = target_table.find('tr')\n",
        "        header_text = [th.get_text(strip=True) for th in headers.find_all('th')]\n",
        "        price_index = header_text.index('Price')\n",
        "        month_index = header_text.index('Month')\n",
        "        rows = target_table.find_all('tr')\n",
        "        gold_row = None\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            if cells and cells[0].get_text(strip=True) == 'Gold':\n",
        "                gold_row = row\n",
        "                break\n",
        "        if not gold_row:\n",
        "            print(\"Gold row not found.\")\n",
        "            return None, None\n",
        "        tds = gold_row.find_all('td')\n",
        "        price_text = tds[price_index].get_text(strip=True)\n",
        "        monthly_change_text = tds[month_index].get_text(strip=True)\n",
        "        try:\n",
        "            price_text = price_text.replace(',', '').replace('$', '')\n",
        "            current_price = float(price_text)\n",
        "            monthly_change_text = monthly_change_text.replace(',', '').replace('+', '').replace('−', '-').replace('%', '')\n",
        "            monthly_change = float(monthly_change_text) / 100 if '%' in tds[month_index].get_text(strip=True) else float(monthly_change_text)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error converting to float: {e}\")\n",
        "            return None, None\n",
        "        return current_price, monthly_change\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching price data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\"\"\"\n",
        "    Getting user input for signals and investment\n",
        "    - I’m asking for signals from 15 days ago to simulate past decisions\n",
        "    - Using numbers to make input easier (1=Buy, 2=Sell, 3=Hold)\n",
        "    \"\"\"\n",
        "current_price, change_15_days = get_price_data(\"GC=F\")\n",
        "if current_price is None or change_15_days is None:\n",
        "    print(\"Unable to proceed with backtesting due to missing price data.\")\n",
        "else:\n",
        "    print(\"Enter the trading signal from 15 days ago for each model (1 = Buy, 2 = Sell, 3 = Hold):\")\n",
        "    signal_distil_u = int(input(\"DistilBERT Undersampled Signal: \"))\n",
        "    signal_distil_c = int(input(\"DistilBERT Class-Weighted Signal: \"))\n",
        "    signal_lstm_u = int(input(\"LSTM Undersampled Signal: \"))\n",
        "    signal_lstm_c = int(input(\"LSTM Class-Weighted Signal: \"))\n",
        "    signal_mini_u = int(input(\"MiniLM Undersampled Signal: \"))\n",
        "    signal_mini_c = int(input(\"MiniLM Class-Weighted Signal: \"))\n",
        "    investment = float(input(\"Enter amount invested (in USD): \"))\n",
        "\n",
        "# Mapping numeric signals to trading actions\n",
        "    signal_map = {1: \"Go Long\", 2: \"Go Short\", 3: \"Hold\"}\n",
        "    signals = {\n",
        "        \"DistilBERT Undersampled\": signal_map[signal_distil_u],\n",
        "        \"DistilBERT Class-Weighted\": signal_map[signal_distil_c],\n",
        "        \"LSTM Undersampled\": signal_map[signal_lstm_u],\n",
        "        \"LSTM Class-Weighted\": signal_map[signal_lstm_c],\n",
        "        \"MiniLM Undersampled\": signal_map[signal_mini_u],\n",
        "        \"MiniLM Class-Weighted\": signal_map[signal_mini_c]\n",
        "    }\n",
        "\n",
        "# Calculating returns using the monthly change as a proxy for 15-day change\n",
        "    price_15_days_ago = current_price / (1 + change_15_days)\n",
        "    print(f\"\\nPrice 15 Days Ago (Estimated): ${price_15_days_ago:,.2f}\")\n",
        "    print(f\"Current Price: ${current_price:,.2f}\")\n",
        "    print(f\"15-Day Change (Using Monthly Proxy): {change_15_days:.2%}\")\n",
        "\n",
        "    for model, signal in signals.items():\n",
        "        print(f\"\\n{model} (Signal: {signal}):\")\n",
        "        if signal == \"Go Long\":\n",
        "            returns = investment * (current_price / price_15_days_ago - 1)\n",
        "            print(f\"Returns: ${returns:,.2f} ({(returns/investment)*100:.2f}%)\")\n",
        "            print(\"Correct\" if change_15_days > 0 else \"Incorrect\")\n",
        "        elif signal == \"Go Short\":\n",
        "            returns = investment * (price_15_days_ago / current_price - 1)\n",
        "            print(f\"Returns: ${returns:,.2f} ({(returns/investment)*100:.2f}%)\")\n",
        "            print(\"Correct\" if change_15_days < 0 else \"Incorrect\")\n",
        "        else:\n",
        "            returns = 0\n",
        "            print(f\"Returns: ${returns:,.2f} (0.00%)\")\n",
        "            print(\"Correct\" if abs(change_15_days) < 0.01 else \"Incorrect\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
